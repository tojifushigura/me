Вот краткое объяснение каждой концепции:

### Теория линейной регрессии
**Линейная регрессия** — это метод статистического анализа, используемый для моделирования зависимости между одной зависимой переменной \( y \) и одной или несколькими независимыми переменными \( x \). Основная цель линейной регрессии — найти наилучшую линейную функцию, которая предсказывает значения \( y \) на основе значений \( x \). Модель можно записать в виде уравнения:
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \varepsilon
\]
где:
- \( y \) — предсказанное значение,
- \( \beta_0 \) — свободный член (интерсепт),
- \( \beta_1, \beta_2, \dots, \beta_n \) — коэффициенты, которые оцениваются,
- \( x_1, x_2, \dots, x_n \) — независимые переменные,
- \( \varepsilon \) — случайная ошибка (разница между наблюдаемыми и предсказанными значениями).

Коэффициенты \( \beta \) находятся методом наименьших квадратов (OLS), при котором минимизируется сумма квадратов отклонений предсказанных значений от реальных значений.

### Теория полиномиальной регрессии
**Полиномиальная регрессия** — это расширение линейной регрессии, в которой зависимость между переменной \( y \) и независимой переменной \( x \) моделируется в виде полинома более высокой степени. Например:
\[
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \dots + \beta_n x^n + \varepsilon
\]
В отличие от линейной регрессии, которая предполагает линейные зависимости, полиномиальная регрессия позволяет захватывать нелинейные зависимости. Она полезна в случаях, когда распределение данных является нелинейным, и простая линейная регрессия не может дать хорошие результаты. Однако при увеличении степени полинома появляется риск **переобучения**, когда модель слишком точно следует обучающей выборке, но плохо обобщает новые данные.

### Теория регуляризации и её виды
**Регуляризация** — это техника, используемая для уменьшения переобучения, добавляя штраф к модели за чрезмерную сложность. Регуляризация изменяет цель оптимизации, добавляя к функции потерь регуляризационный член, который контролирует величину коэффициентов модели. Существует несколько видов регуляризации:

1. **L2-регуляризация (Ridge-регрессия)**: добавляет штраф на сумму квадратов коэффициентов:
   \[
   \text{Функция потерь} = \sum (y_i - \hat{y_i})^2 + \alpha \sum \beta_i^2
   \]
   где \( \alpha \) — гиперпараметр регуляризации. Этот метод предотвращает чрезмерное увеличение коэффициентов, уменьшает дисперсию и часто используется для снижения коллинеарности.

2. **L1-регуляризация (Lasso-регрессия)**: добавляет штраф на сумму абсолютных значений коэффициентов:
   \[
   \text{Функция потерь} = \sum (y_i - \hat{y_i})^2 + \alpha \sum |\beta_i|
   \]
   Она также уменьшает вес коэффициентов, но в отличие от Ridge-регрессии, может сводить некоторые из них к нулю, что автоматически выполняет отбор признаков, убирая несущественные параметры.

3. **Elastic Net**: комбинирует L1 и L2 регуляризации:
   \[
   \text{Функция потерь} = \sum (y_i - \hat{y_i})^2 + \alpha_1 \sum |\beta_i| + \alpha_2 \sum \beta_i^2
   \]
   где \( \alpha_1 \) и \( \alpha_2 \) — коэффициенты L1 и L2 регуляризации соответственно. Elastic Net применяется, когда имеются многократные коррелированные признаки, и одна регуляризация не справляется с отбором признаков.

Регуляризация используется для улучшения обобщающей способности модели, уменьшая её склонность к переобучению и делая её более устойчивой к коллинеарности.

### Импорт библиотек

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from ucimlrepo import fetch_ucirepo
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
```

1. **numpy** (`np`) и **pandas** (`pd`): используются для работы с массивами и структурами данных.
2. **matplotlib.pyplot** (`plt`): для построения графиков.
3. **ucimlrepo.fetch_ucirepo**: библиотека для получения данных из репозитория UCI Machine Learning.
4. **sklearn.linear_model.LinearRegression** и **sklearn.linear_model.Ridge**: классы для выполнения линейной регрессии и Ridge-регрессии.
5. **PolynomialFeatures**: для преобразования данных в полиномиальные признаки.
6. **mean_squared_error** и **r2_score**: для оценки качества модели.

### Загрузка набора данных

```python
energy_efficiency = fetch_ucirepo(id=242)
X = energy_efficiency.data.features
y = energy_efficiency.data.targets
```

1. `fetch_ucirepo(id=242)`: загружает данные о энергоэффективности из UCI по ID.
2. `X` и `y`: соответственно признаки (независимые переменные) и целевые значения (зависимые переменные).

### 1. Разделение данных на обучающую и тестовую выборки

```python
np.random.seed(42)  # Для воспроизводимости
indices = np.random.permutation(len(X))
train_size = int(len(X) * 0.8)
train_indices = indices[:train_size]
test_indices = indices[train_size:]

X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]
y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]
```

1. `np.random.seed(42)`: фиксирует случайное начальное значение, чтобы данные разделялись одинаково при каждом запуске.
2. `indices`: создает перемешанный массив индексов.
3. `train_size`: вычисляет количество данных для обучения (80% от общего количества).
4. `train_indices` и `test_indices`: индексы для тренировочной и тестовой выборок.
5. `X_train`, `X_test`, `y_train`, `y_test`: деление данных на тренировочную и тестовую выборки.

### 2. Обучение модели линейной регрессии

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

y_pred = lin_reg.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Линейная регрессия: MSE = {mse:.2f}, R² = {r2:.2f}")
```

1. `lin_reg = LinearRegression()`: создание модели линейной регрессии.
2. `lin_reg.fit(X_train, y_train)`: обучение модели на тренировочных данных.
3. `y_pred = lin_reg.predict(X_test)`: предсказание на тестовой выборке.
4. `mse` и `r2`: вычисление средней квадратичной ошибки (MSE) и коэффициента детерминации (R²), которые оценивают точность модели.
5. `print`: выводит результаты.

### 3. Построение полиномиальной модели

```python
degrees = [1, 2, 3, 4, 5]
train_scores = []
test_scores = []

for degree in degrees:
    poly_features = PolynomialFeatures(degree=degree)
    X_poly_train = poly_features.fit_transform(X_train)
    X_poly_test = poly_features.transform(X_test)
    
    poly_reg = LinearRegression()
    poly_reg.fit(X_poly_train, y_train)
    
    train_pred = poly_reg.predict(X_poly_train)
    test_pred = poly_reg.predict(X_poly_test)
    
    train_scores.append(r2_score(y_train, train_pred))
    test_scores.append(r2_score(y_test, test_pred))
```

1. **Цикл по степеням `degrees`**: для построения модели с разными степенями полинома.
2. `poly_features`: трансформирует признаки в полиномиальные.
3. `X_poly_train`, `X_poly_test`: преобразованные тренировочные и тестовые данные.
4. `poly_reg.fit`, `poly_reg.predict`: обучение и предсказание для полиномиальной модели.
5. `train_scores` и `test_scores`: добавление оценок для каждой степени полинома.

#### Построение графика

```python
plt.figure(figsize=(10, 5))
plt.plot(degrees, train_scores, label='Train R²', marker='o')
plt.plot(degrees, test_scores, label='Test R²', marker='o')
plt.title('Точность полиномиальной регрессии в зависимости от степени полинома')
plt.xlabel('Степень полинома')
plt.ylabel('R²')
plt.legend()
plt.grid()
plt.show()
```

1. `plt.plot`: строит графики точности модели в зависимости от степени полинома.
2. `plt.legend`, `plt.grid`, `plt.show()`: настройка и отображение графика.

### 4. Модель с регуляризацией (Ridge Regression)

```python
ridge_train_scores = []
ridge_test_scores = []
alphas = np.logspace(-4, 4, 10)

for alpha in alphas:
    ridge_reg = Ridge(alpha=alpha)
    ridge_reg.fit(X_train, y_train)
    
    ridge_train_pred = ridge_reg.predict(X_train)
    ridge_test_pred = ridge_reg.predict(X_test)
    
    ridge_train_scores.append(r2_score(y_train, ridge_train_pred))
    ridge_test_scores.append(r2_score(y_test, ridge_test_pred))
```

1. **Цикл по `alphas`**: создает и оценивает модели Ridge-регрессии с разными коэффициентами регуляризации.
2. `ridge_reg`: создается модель с Ridge-регрессией.
3. `ridge_train_scores` и `ridge_test_scores`: сохраняет точности для тренировочной и тестовой выборок.

#### Построение графика зависимости от коэффициента регуляризации

```python
plt.figure(figsize=(10, 5))
plt.plot(alphas, ridge_train_scores, label='Train R²', marker='o')
plt.plot(alphas, ridge_test_scores, label='Test R²', marker='o')
plt.xscale('log')
plt.title('Точность Ridge-регрессии в зависимости от коэффициента регуляризации')
plt.xlabel('Коэффициент регуляризации (alpha)')
plt.ylabel('R²')
plt.legend()
plt.grid()
plt.show()
```

1. `plt.xscale('log')`: логарифмическая шкала по оси X для коэффициента регуляризации.
2. `plt.plot`: графики зависимости точности от коэффициента регуляризации.
