### **Методы оптимизации в машинном обучении**

Оптимизация — это процесс нахождения таких параметров модели (например, весов), которые минимизируют функцию потерь. Различные методы оптимизации используются для достижения этого.

---

#### **1. Stochastic Gradient Descent (SGD)**

**Описание:**  
SGD — это упрощенная версия градиентного спуска. Вместо вычисления градиента на всем наборе данных, он обновляет параметры на основе одного случайного примера или небольшого подмножества (батча).  

**Основные характеристики:**  
- Быстрее, чем обычный градиентный спуск, при больших объемах данных.  
- Может "прыгать" через локальные минимумы, что иногда помогает найти лучшее глобальное решение.  

**Плюсы:**  
- Легко реализуется.  
- Эффективен на больших данных.  

**Минусы:**  
- Шум в обновлении может мешать сходимости.  
- Требует настройки скорости обучения (`learning_rate`).

---

#### **2. Adaptive Moment Estimation (Adam)**

**Описание:**  
Adam объединяет идеи двух методов:  
- **Momentum** — ускоряет обучение, учитывая предыдущие градиенты.  
- **RMSProp** — нормализует градиенты, используя их масштабы.  

Adam адаптирует скорость обучения для каждого параметра с учетом его изменений в прошлом.  

**Основные характеристики:**  
- Использует два параметра: **\( \beta_1 \)** и **\( \beta_2 \)**, которые контролируют скорость изменения моментов.  
- Обновление параметров основывается на скользящем среднем градиентов и их квадратов.

**Плюсы:**  
- Автоматическая адаптация скорости обучения.  
- Эффективен для негладких функций потерь.  
- Хорошо работает на сложных задачах.  

**Минусы:**  
- Может переобучиться, если данные шумные.  
- Требует настройки гиперпараметров.

---

#### **3. Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)**

**Описание:**  
Это метод второго порядка (использует информацию о втором производном, т.е. матрице Гессе). L-BFGS работает как улучшенная версия градиентного спуска, оптимизируя вычисления для высокоразмерных задач.  

**Основные характеристики:**  
- Аппроксимирует матрицу Гессе, не вычисляя ее напрямую.  
- Используется в задачах, где число параметров большое.  

**Плюсы:**  
- Быстро сходится для выпуклых функций.  
- Меньше подвержен шуму, чем SGD.  

**Минусы:**  
- Требует больше памяти, чем SGD или Adam.  
- Может быть менее эффективным для задач с большим объемом данных.

---

### **Сравнение методов**

| **Метод**      | **Применение**                 | **Плюсы**                     | **Минусы**                     | **Тип данных**            |
|-----------------|--------------------------------|--------------------------------|---------------------------------|---------------------------|
| **SGD**         | Линейные модели, большие данные| Простота, низкие затраты       | Высокий шум, сложность настройки| Подходит для больших данных |
| **Adam**        | Глубокие нейронные сети        | Быстрая сходимость, адаптивность| Чувствительность к параметрам  | Для сложных задач и сетей |
| **L-BFGS**      | Логистическая регрессия, SVM   | Быстрая сходимость, точность   | Высокая память, не подходит для больших данных| Выпуклые задачи |

**Итог:**  
- **SGD** — прост, эффективен на больших данных, но требует настройки скорости обучения.  
- **Adam** — лучше всего подходит для нейронных сетей, так как адаптивен.  
- **L-BFGS** — предпочтителен для задач с малым числом параметров или выпуклых функций.
