### **Теория**

1. **Масштабирование признаков**  
   Приведение данных к единому масштабу (например, среднее 0, стандартное отклонение 1) для устранения влияния разного масштаба признаков на обучение моделей. Используется для улучшения сходимости алгоритмов.

2. **Модель Perceptron**  
   Линейный классификатор, который ищет гиперплоскость для разделения классов. Это базовый алгоритм машинного обучения. Гиперпараметры:  
   - `max_iter`: максимальное число итераций обучения.  
   - `tol`: порог для остановки обучения.

3. **Модель MLPClassifier**  
   Многослойный перцептрон (искусственная нейронная сеть). Подходит для нелинейных задач. Гиперпараметры:  
   - `hidden_layer_sizes`: архитектура скрытых слоев (например, количество нейронов).  
   - `learning_rate_init`: начальная скорость обучения.  
   - `alpha`: коэффициент регуляризации.  
   - `max_iter`: максимальное число итераций.

4. **Гиперпараметры**  
   Настраиваемые параметры модели, которые задаются до обучения. Примеры: скорость обучения, количество нейронов, коэффициент регуляризации.

5. **Точность (Accuracy)**  
   Метрика, показывающая долю правильно классифицированных примеров:
   \[
   \text{Accuracy} = \frac{\text{Число правильных предсказаний}}{\text{Общее число примеров}}
   \]

---

### **Цифры из кода**

1. **`id=94`**: идентификатор набора данных Spambase в репозитории UCI.  
2. **`test_size=0.4` и `test_size=0.5`**: доли данных для валидации и тестирования.  
3. **`max_iter=1000` и `tol=1e-3`**: максимальное число итераций и порог для остановки в `Perceptron`.  
4. **`hidden_layer_sizes=(100,)`**: 1 скрытый слой с 100 нейронами в `MLPClassifier`.  
5. **`learning_rate_init=[0.001, 0.01, 0.1, 0.5]`**: начальные скорости обучения для экспериментов.  
6. **`alpha=[0.0001, 0.001, 0.01, 0.1]`**: коэффициенты регуляризации для экспериментов.  
7. **Разделение данных:**
   - 60% на обучение, 20% на валидацию, 20% на тестирование.
